"""Script de test pour vLLM avec le mod√®le Granite."""

from vllm import LLM, SamplingParams


def run_inference():
    """Test basique de vLLM avec le mod√®le Granite."""
    llm = LLM(
        model="ibm-granite/granite-3.1-8b-instruct",
        max_model_len=2048,
        max_num_batched_tokens=2048,
    )
    sampling_params = SamplingParams(temperature=0.7, top_p=0.9, max_tokens=100)

    prompts = [
        "Salut, peux-tu m'expliquer ce qu'est vLLM ?",
        "√âcris-moi une fonction Python qui calcule la factorielle d'un nombre.",
        "Quels sont les avantages des mod√®les de langage open-source ?",
    ]
    print("\nüî§ G√©n√©ration de r√©ponses...")
    for i, prompt in enumerate(prompts, 1):
        print(f"\n--- Test {i} ---")
        print(f"Prompt: {prompt}")

        outputs = llm.generate(prompt, sampling_params)

        generated_text = outputs[0].outputs[0].text
        print(f"R√©ponse: {generated_text}")


if __name__ == "__main__":
    run_inference()
